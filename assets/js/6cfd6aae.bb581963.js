"use strict";(self.webpackChunkbento=self.webpackChunkbento||[]).push([[711],{95140:function(e,n,t){t.r(n),t.d(n,{contentTitle:function(){return i},default:function(){return h},frontMatter:function(){return r},metadata:function(){return c},toc:function(){return a}});var o=t(85893),s=t(11151);const r={slug:"enrichments",title:"Enrichment Workflows",description:"How to configure Bento to process a workflow of enrichment services."},i=void 0,c={permalink:"/cookbooks/enrichments",source:"@site/cookbooks/enrichments.md",title:"Enrichment Workflows",description:"How to configure Bento to process a workflow of enrichment services."},a=[{value:"Meet the Enrichments",id:"meet-the-enrichments",level:2},{value:"Claims Detector",id:"claims-detector",level:3},{value:"Hyperbole Detector",id:"hyperbole-detector",level:3},{value:"Fake News Detector",id:"fake-news-detector",level:3},{value:"Combining into a Workflow",id:"combining-into-a-workflow",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",...(0,s.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["This cookbook demonstrates how to enrich a stream of JSON documents with HTTP services. This method also works with ",(0,o.jsx)(n.a,{href:"/docs/components/processors/aws_lambda",children:"AWS Lambda functions"}),", ",(0,o.jsx)(n.a,{href:"/docs/components/processors/subprocess",children:"subprocesses"}),", etc."]}),"\n",(0,o.jsxs)(n.p,{children:["We will start off by configuring a single enrichment, then we will move onto a workflow of enrichments with a network of dependencies using the ",(0,o.jsxs)(n.a,{href:"/docs/components/processors/workflow",children:[(0,o.jsx)(n.code,{children:"workflow"})," processor"]}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["Each enrichment will be performed in parallel across a ",(0,o.jsx)(n.a,{href:"/docs/configuration/batching",children:"pre-batched"})," stream of documents. Workflow enrichments that do not depend on each other will also be performed in parallel, making this orchestration method very efficient."]}),"\n",(0,o.jsxs)(n.p,{children:["The imaginary problem we are going to solve is applying a set of NLP based enrichments to a feed of articles in order to detect fake news. We will be consuming and writing to Kafka, but the example works with any ",(0,o.jsx)(n.a,{href:"/docs/components/inputs/about",children:"input"})," and ",(0,o.jsx)(n.a,{href:"/docs/components/outputs/about",children:"output"})," combination."]}),"\n",(0,o.jsxs)(n.p,{children:["Articles are received over the topic ",(0,o.jsx)(n.code,{children:"articles"})," and look like this:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "type": "article",\n  "article": {\n    "id": "123foo",\n    "title": "Dogs Stop Barking",\n    "content": "The world was shocked this morning to find that all dogs have stopped barking."\n  }\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"meet-the-enrichments",children:"Meet the Enrichments"}),"\n",(0,o.jsx)(n.h3,{id:"claims-detector",children:"Claims Detector"}),"\n",(0,o.jsx)(n.p,{children:"To start us off we will configure a single enrichment, which is an imaginary 'claims detector' service. This is an HTTP service that wraps a trained machine learning model to extract claims that are made within a body of text."}),"\n",(0,o.jsxs)(n.p,{children:["The service expects a ",(0,o.jsx)(n.code,{children:"POST"})," request with JSON payload of the form:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "text": "The world was shocked this morning to find that all dogs have stopped barking."\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:"And returns a JSON payload of the form:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "claims": [\n    {\n      "entity": "world",\n      "claim": "shocked"\n    },\n    {\n      "entity": "dogs",\n      "claim": "NOT barking"\n    }\n  ]\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:"Since each request only applies to a single document we will make this enrichment scale by deploying multiple HTTP services and hitting those instances in parallel across our document batches."}),"\n",(0,o.jsxs)(n.p,{children:["In order to send a mapped request and map the response back into the original document we will use the ",(0,o.jsxs)(n.a,{href:"/docs/components/processors/branch",children:[(0,o.jsx)(n.code,{children:"branch"})," processor"]}),", with a child ",(0,o.jsx)(n.a,{href:"/docs/components/processors/http",children:(0,o.jsx)(n.code,{children:"http"})})," processor."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"input:\n  kafka:\n    addresses: [ TODO ]\n    topics: [ articles ]\n    consumer_group: bento_articles_group\n    batching:\n      count: 20 # Tune this to set the size of our document batches.\n      period: 1s\n\npipeline:\n  processors:\n    - branch:\n        request_map: 'root.text = this.article.content'\n        processors:\n          - http:\n              url: http://localhost:4197/claims\n              verb: POST\n        result_map: 'root.tmp.claims = this.claims'\n\noutput:\n  kafka:\n    addresses: [ TODO ]\n    topic: comments_hydrated\n"})}),"\n",(0,o.jsx)(n.p,{children:"With this pipeline our documents will come out looking something like this:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "type": "article",\n  "article": {\n    "id": "123foo",\n    "title": "Dogs Stop Barking",\n    "content": "The world was shocked this morning to find that all dogs have stopped barking."\n  },\n  "tmp": {\n    "claims": [\n      {\n        "entity": "world",\n        "claim": "shocked"\n      },\n      {\n        "entity": "dogs",\n        "claim": "NOT barking"\n      }\n    ]\n  }\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"hyperbole-detector",children:"Hyperbole Detector"}),"\n",(0,o.jsxs)(n.p,{children:["Next up is a 'hyperbole detector' that takes a ",(0,o.jsx)(n.code,{children:"POST"})," request containing the article contents and returns a hyperbole score between 0 and 1. This time the format is array-based and therefore supports calculating multiple documents in a single request, making better use of the host machines GPU."]}),"\n",(0,o.jsx)(n.p,{children:"A request should take the following form:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "text": "The world was shocked this morning to find that all dogs have stopped barking."\n  }\n]\n'})}),"\n",(0,o.jsx)(n.p,{children:"And the response looks like this:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "hyperbole_rank": 0.73\n  }\n]\n'})}),"\n",(0,o.jsxs)(n.p,{children:["In order to create a single request from a batch of documents, and subsequently map the result back into our batch, we will use the ",(0,o.jsx)(n.a,{href:"/docs/components/processors/archive",children:(0,o.jsx)(n.code,{children:"archive"})})," and ",(0,o.jsx)(n.a,{href:"/docs/components/processors/unarchive",children:(0,o.jsx)(n.code,{children:"unarchive"})})," processors in our ",(0,o.jsx)(n.a,{href:"/docs/components/processors/branch",children:(0,o.jsx)(n.code,{children:"branch"})})," flow, like this:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"pipeline:\n  processors:\n    - branch:\n        request_map: 'root.text = this.article.content'\n        processors:\n          - archive:\n              format: json_array\n          - http:\n              url: http://localhost:4198/hyperbole\n              verb: POST\n          - unarchive:\n              format: json_array\n        result_map: 'root.tmp.hyperbole_rank = this.hyperbole_rank'\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The purpose of the ",(0,o.jsx)(n.code,{children:"json_array"})," format ",(0,o.jsx)(n.code,{children:"archive"})," processor is to take a batch of JSON documents and place them into a single document as an array. Subsequently, we then send one single request for each batch."]}),"\n",(0,o.jsxs)(n.p,{children:["After the request is made we do the opposite with the ",(0,o.jsx)(n.code,{children:"unarchive"})," processor in order to convert it back into a batch of the original size."]}),"\n",(0,o.jsx)(n.h3,{id:"fake-news-detector",children:"Fake News Detector"}),"\n",(0,o.jsx)(n.p,{children:"Finally, we are going to use a 'fake news detector' that takes the article contents as well as the output of the previous two enrichments and calculates a fake news rank between 0 and 1."}),"\n",(0,o.jsx)(n.p,{children:"This service behaves similarly to the claims detector service and takes a document of the form:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "text": "The world was shocked this morning to find that all dogs have stopped barking.",\n  "hyperbole_rank": 0.73,\n  "claims": [\n    {\n      "entity": "world",\n      "claim": "shocked"\n    },\n    {\n      "entity": "dogs",\n      "claim": "NOT barking"\n    }\n  ]\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:"And returns an object of the form:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "fake_news_rank": 0.893\n}\n'})}),"\n",(0,o.jsxs)(n.p,{children:["We then wish to map the field ",(0,o.jsx)(n.code,{children:"fake_news_rank"})," from that result into the original document at the path ",(0,o.jsx)(n.code,{children:"article.fake_news_score"}),". Our ",(0,o.jsx)(n.a,{href:"/docs/components/processors/branch",children:(0,o.jsx)(n.code,{children:"branch"})})," block for this enrichment would look like this:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"pipeline:\n  processors:\n    - branch:\n        request_map: |\n          root.text = this.article.content\n          root.claims = this.tmp.claims\n          root.hyperbole_rank = this.tmp.hyperbole_rank\n        processors:\n          - http:\n              url: http://localhost:4199/fakenews\n              verb: POST\n        result_map: 'root.article.fake_news_score = this.fake_news_rank'\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Note that in our ",(0,o.jsx)(n.code,{children:"request_map"})," we are targeting fields that are populated from the previous two enrichments."]}),"\n",(0,o.jsx)(n.p,{children:"If we were to execute all three enrichments in a sequence we'll end up with a document looking like this:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "type": "article",\n  "article": {\n    "id": "123foo",\n    "title": "Dogs Stop Barking",\n    "content": "The world was shocked this morning to find that all dogs have stopped barking.",\n    "fake_news_rank": 0.76\n  },\n  "tmp": {\n    "hyperbole_rank": 0.34,\n    "claims": [\n      {\n        "entity": "world",\n        "claim": "shocked"\n      },\n      {\n        "entity": "dogs",\n        "claim": "NOT barking"\n      }\n    ]\n  }\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:"Great! However, as a streaming pipeline this set up isn't ideal as our first two enrichments are independent and could potentially be executed in parallel in order to reduce processing latency."}),"\n",(0,o.jsx)(n.h2,{id:"combining-into-a-workflow",children:"Combining into a Workflow"}),"\n",(0,o.jsxs)(n.p,{children:["If we configure our enrichments within a ",(0,o.jsxs)(n.a,{href:"/docs/components/processors/workflow",children:[(0,o.jsx)(n.code,{children:"workflow"})," processor"]})," we can use Bento to automatically detect our dependency graph, giving us two key benefits:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Enrichments at the same level of a dependency graph (claims and hyperbole) will be executed in parallel."}),"\n",(0,o.jsx)(n.li,{children:"When introducing more enrichments to our pipeline the added complexity of resolving the dependency graph is handled automatically by Bento."}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Placing our branches within a ",(0,o.jsxs)(n.a,{href:"/docs/components/processors/workflow",children:[(0,o.jsx)(n.code,{children:"workflow"})," processor"]})," makes our final pipeline configuration look like this:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"input:\n  kafka:\n    addresses: [ TODO ]\n    topics: [ articles ]\n    consumer_group: bento_articles_group\n    batching:\n      count: 20 # Tune this to set the size of our document batches.\n      period: 1s\n\npipeline:\n  processors:\n    - workflow:\n        meta_path: '' #\xa0Don't bother storing branch metadata.\n        branches:\n          claims:\n            request_map: 'root.text = this.article.content'\n            processors:\n              - http:\n                  url: http://localhost:4197/claims\n                  verb: POST\n            result_map: 'root.tmp.claims = this.claims'\n\n          hyperbole:\n            request_map: 'root.text = this.article.content'\n            processors:\n              - archive:\n                  format: json_array\n              - http:\n                  url: http://localhost:4198/hyperbole\n                  verb: POST\n              - unarchive:\n                  format: json_array\n            result_map: 'root.tmp.hyperbole_rank = this.hyperbole_rank'\n\n          fake_news:\n            request_map: |\n              root.text = this.article.content\n              root.claims = this.tmp.claims\n              root.hyperbole_rank = this.tmp.hyperbole_rank\n            processors:\n              - http:\n                  url: http://localhost:4199/fakenews\n                  verb: POST\n            result_map: 'root.article.fake_news_score = this.fake_news_rank'\n\n    - catch:\n        - log:\n            fields_mapping: 'root.content = content().string()'\n            message: \"Enrichments failed due to: ${!error()}\"\n\n    - mapping: |\n        root = this\n        root.tmp = deleted()\n\noutput:\n  kafka:\n    addresses: [ TODO ]\n    topic: comments_hydrated\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Since the contents of ",(0,o.jsx)(n.code,{children:"tmp"})," won't be required downstream we remove it after our enrichments using a ",(0,o.jsxs)(n.a,{href:"/docs/components/processors/mapping",children:[(0,o.jsx)(n.code,{children:"mapping"})," processor"]}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["A ",(0,o.jsx)(n.a,{href:"/docs/components/processors/catch",children:(0,o.jsx)(n.code,{children:"catch"})})," processor was added at the end of the pipeline which catches documents that failed enrichment. You can replace the log event with a wide range of recovery actions such as sending to a dead-letter/retry queue, dropping the message entirely, etc. You can read more about error handling ",(0,o.jsx)(n.a,{href:"/docs/configuration/error_handling",children:"in this article"}),"."]})]})}function h(e){void 0===e&&(e={});const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},11151:function(e,n,t){t.d(n,{Z:function(){return c},a:function(){return i}});var o=t(67294);const s={},r=o.createContext(s);function i(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);