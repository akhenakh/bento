"use strict";(self.webpackChunkbento=self.webpackChunkbento||[]).push([[5852],{2661:function(e,t,n){n.r(t),n.d(t,{assets:function(){return u},contentTitle:function(){return r},default:function(){return h},frontMatter:function(){return i},metadata:function(){return a},toc:function(){return c}});var o=n(85893),s=n(11151);const i={title:"Performance Tuning"},r=void 0,a={id:"guides/performance_tuning",title:"Performance Tuning",description:"Maximising IO Throughput",source:"@site/docs/guides/performance_tuning.md",sourceDirName:"guides",slug:"/guides/performance_tuning",permalink:"/docs/guides/performance_tuning",draft:!1,unlisted:!1,editUrl:"https://github.com/warpstreamlabs/bento/edit/main/website/docs/guides/performance_tuning.md",tags:[],version:"current",frontMatter:{title:"Performance Tuning"},sidebar:"docs",previous:{title:"Monitoring",permalink:"/docs/guides/monitoring"},next:{title:"Synchronous Responses",permalink:"/docs/guides/sync_responses"}},u={},c=[{value:"Maximising IO Throughput",id:"maximising-io-throughput",level:2},{value:"Bento Reads Too Slowly",id:"bento-reads-too-slowly",level:3},{value:"Bento Writes Too Slowly",id:"bento-writes-too-slowly",level:3},{value:"Increase in flight messages",id:"increase-in-flight-messages",level:4},{value:"Send messages in batches",id:"send-messages-in-batches",level:4},{value:"Level out input spikes with a buffer",id:"level-out-input-spikes-with-a-buffer",level:4},{value:"Maximising CPU Utilisation",id:"maximising-cpu-utilisation",level:2}];function l(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",...(0,s.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h2,{id:"maximising-io-throughput",children:"Maximising IO Throughput"}),"\n",(0,o.jsx)(t.p,{children:"This section outlines a few common throughput issues and ways in which they can be solved within Bento."}),"\n",(0,o.jsxs)(t.p,{children:["It is assumed here that your Bento instance is performing only minor processing steps, and therefore has minimal reliance on your CPU resource. If this is not the case the following still applies to an extent, but you should also refer to ",(0,o.jsx)(t.a,{href:"#maximising-cpu-utilisation",children:"the next section regarding CPU utilisation"}),"."]}),"\n",(0,o.jsx)(t.p,{children:"Firstly, before venturing into Bento configurations, you should take an in-depth look at your sources and sinks. Bento is generally much simpler architecturally than the inputs and outputs it supports. Spend some time understanding how to squeeze the most out of these services and it will make it easier (or unnecessary) to tune your Bento configuration."}),"\n",(0,o.jsx)(t.h3,{id:"bento-reads-too-slowly",children:"Bento Reads Too Slowly"}),"\n",(0,o.jsxs)(t.p,{children:["If Bento isn't reading fast enough from your source it might not necessarily be due to a slow consumer. If the sink is slow this can cause back pressure that throttles the amount Bento can read. Try consuming a test feed with the output replaced with ",(0,o.jsx)(t.code,{children:"drop"}),". If you notice that the input consumption suddenly speeds up then the issue is likely with the output, in which case ",(0,o.jsx)(t.a,{href:"#bento-writes-too-slowly",children:"try the next section"}),"."]}),"\n",(0,o.jsxs)(t.p,{children:["If the ",(0,o.jsx)(t.code,{children:"drop"})," output pipe didn't help then take a quick look at the basic configuration fields for the input source type. Sometimes there are fields for setting a number of background prefetches or similar concepts that can increase your throughput. For example, increasing the value of ",(0,o.jsx)(t.code,{children:"prefetch_count"})," for an AMQP consumer can greatly increase the rate at which it is consumed."]}),"\n",(0,o.jsxs)(t.p,{children:["Next, if your source supports multiple parallel consumers then you can try doing that within Bento by using a ",(0,o.jsx)(t.a,{href:"/docs/components/inputs/broker",children:"broker"}),". For example, if you started with:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-yaml",children:"input:\n  http_client:\n    url: http://localhost:4195/get\n    verb: GET\n"})}),"\n",(0,o.jsx)(t.p,{children:"You could change to:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-yaml",children:"input:\n  broker:\n    copies: 4\n    inputs:\n      - http_client:\n          url: http://localhost:4195/get\n          verb: GET\n"})}),"\n",(0,o.jsxs)(t.p,{children:["Which would create the exact same consumer as before with four connections in total. Try increasing the number of copies to see how that affects the throughput. If your multiple consumers would require different configurations then set copies to ",(0,o.jsx)(t.code,{children:"1"})," and write each consumer as a separate object in the ",(0,o.jsx)(t.code,{children:"inputs"})," array."]}),"\n",(0,o.jsxs)(t.p,{children:["Read the ",(0,o.jsx)(t.a,{href:"/docs/components/inputs/broker",children:"broker documentation"})," for more tips on simplifying broker configs."]}),"\n",(0,o.jsx)(t.p,{children:"If your source doesn't support multiple parallel consumers then unfortunately your options are more limited. A logical next step might be to look at your network/disk configuration to see if that's a potential cause of contention."}),"\n",(0,o.jsx)(t.h3,{id:"bento-writes-too-slowly",children:"Bento Writes Too Slowly"}),"\n",(0,o.jsx)(t.p,{children:"If you have an output sink that regularly places back pressure on your source there are a few solutions depending on the details of the issue."}),"\n",(0,o.jsxs)(t.p,{children:["Firstly, you should check the config parameters of your output sink. There are often fields specifically for controlling the level of acknowledgement to expect before moving onto the next message, if these levels of guarantee are overkill you can disable them for greater throughput. For example, setting the ",(0,o.jsx)(t.code,{children:"ack_replicas"})," field to ",(0,o.jsx)(t.code,{children:"false"})," in the Kafka sink can have a high impact on throughput."]}),"\n",(0,o.jsx)(t.p,{children:"If the config parameters for an output sink aren't enough then you can try the following:"}),"\n",(0,o.jsx)(t.h4,{id:"increase-in-flight-messages",children:"Increase in flight messages"}),"\n",(0,o.jsxs)(t.p,{children:["Most outputs have a field ",(0,o.jsx)(t.code,{children:"max_in_flight"})," that allows you to specify how many messages can be in flight at the same time. Increasing this value can improve throughput significantly."]}),"\n",(0,o.jsx)(t.h4,{id:"send-messages-in-batches",children:"Send messages in batches"}),"\n",(0,o.jsxs)(t.p,{children:["Most outputs will send data quicker when messages are batched, this is often done automatically in the background. However, for a few outputs your batches need to be configured. Read the ",(0,o.jsx)(t.a,{href:"/docs/configuration/batching",children:"batching documentation"})," for more guidance on how to tune message batches within Bento."]}),"\n",(0,o.jsx)(t.h4,{id:"level-out-input-spikes-with-a-buffer",children:"Level out input spikes with a buffer"}),"\n",(0,o.jsx)(t.p,{children:"There are many reasons why an input source might have spikes or inconsistent throughput rates. It is possible that your output is capable of keeping up with\nthe long term average flow of data, but fails to keep up when an intermittent spike occurs."}),"\n",(0,o.jsx)(t.p,{children:"In situations like these it is sometimes a better use of your hardware and resources to level out the flow of data rather than try and match the peak throughput. This would depend on the frequency and duration of the spikes as well as your latency requirements, and is therefore a matter of judgement."}),"\n",(0,o.jsxs)(t.p,{children:["Leveling out the flow of data can be done within Bento using a ",(0,o.jsx)(t.a,{href:"/docs/components/buffers/about",children:"buffer"}),". Buffers allow an input source to store a bounded amount of data temporarily, which a consumer can work through at its own pace. Buffers always have a fixed capacity, which when full will proceed to block the input just like a busy output would."]}),"\n",(0,o.jsxs)(t.p,{children:["Therefore, it's still important to have an output that can keep up with the flow of data, the difference that a buffer makes is that the output only needs to keep up with the ",(0,o.jsx)(t.em,{children:"average"})," flow of data versus the instantaneous flow of data."]}),"\n",(0,o.jsx)(t.p,{children:"For example, if your input usually produces 10 msgs/s, but occasionally spikes to 100 msgs/s, and your output can handle up to 50 msgs/s, it might be possible to configure a buffer large enough to store spikes in their entirety. As long as the average flow of messages from the input remains below 50 msgs/s then your service should be able to continue indefinitely without ever blocking the input source."}),"\n",(0,o.jsx)(t.h2,{id:"maximising-cpu-utilisation",children:"Maximising CPU Utilisation"}),"\n",(0,o.jsxs)(t.p,{children:["Some ",(0,o.jsx)(t.a,{href:"/docs/components/processors/about",children:"processors"})," within Bento are relatively heavy on your CPU, and can potentially become the bottleneck of a service. In these circumstances it is worth configuring Bento so that your processors are running on each available core of your machine without contention."]}),"\n",(0,o.jsxs)(t.p,{children:["An array of processors in any section of a Bento config becomes a single logical pipeline of steps running on a single logical thread. The easiest way to create parallel processor threads is to configure them inside the ",(0,o.jsx)(t.a,{href:"/docs/configuration/processing_pipelines",children:"pipeline"})," configuration block, where we can explicitly set any number of parallel processor threads independent of how many inputs or outputs we want to use."]}),"\n",(0,o.jsxs)(t.p,{children:["Please refer ",(0,o.jsx)(t.a,{href:"/docs/configuration/processing_pipelines",children:"to the documentation regarding pipelines"})," for some examples."]})]})}function h(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},11151:function(e,t,n){n.d(t,{Z:function(){return a},a:function(){return r}});var o=n(67294);const s={},i=o.createContext(s);function r(e){const t=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:t},e.children)}}}]);