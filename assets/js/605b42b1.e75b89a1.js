"use strict";(self.webpackChunkbento=self.webpackChunkbento||[]).push([[2214],{19744:function(e,s,n){n.r(s),n.d(s,{assets:function(){return c},contentTitle:function(){return r},default:function(){return d},frontMatter:function(){return i},metadata:function(){return a},toc:function(){return h}});var o=n(85893),t=n(11151);const i={title:"Message Batching"},r=void 0,a={id:"configuration/batching",title:"Message Batching",description:"Bento is able to join sources and sinks with sometimes conflicting batching behaviours without sacrificing its strong delivery guarantees. It's also able to perform powerful processing functions across batches of messages such as grouping, archiving and reduction. Therefore, batching within Bento is a mechanism that serves multiple purposes:",source:"@site/docs/configuration/batching.md",sourceDirName:"configuration",slug:"/configuration/batching",permalink:"/docs/configuration/batching",draft:!1,unlisted:!1,editUrl:"https://github.com/warpstreamlabs/bento/edit/main/website/docs/configuration/batching.md",tags:[],version:"current",frontMatter:{title:"Message Batching"},sidebar:"docs",previous:{title:"Resources",permalink:"/docs/configuration/resources"},next:{title:"Windowed Processing",permalink:"/docs/configuration/windowed_processing"}},c={},h=[{value:"Performance",id:"performance",level:2},{value:"Grouped Message Processing",id:"grouped-message-processing",level:2},{value:"Compatibility",id:"compatibility",level:2},{value:"Shrinking Batches",id:"shrinking-batches",level:3},{value:"Batch Policy",id:"batch-policy",level:2},{value:"Post-Batch Processing",id:"post-batch-processing",level:3}];function l(e){const s={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(s.p,{children:["Bento is able to join sources and sinks with sometimes conflicting batching behaviours without sacrificing its strong delivery guarantees. It's also able to perform powerful ",(0,o.jsx)(s.a,{href:"/docs/configuration/windowed_processing",children:"processing functions"})," across batches of messages such as grouping, archiving and reduction. Therefore, batching within Bento is a mechanism that serves multiple purposes:"]}),"\n",(0,o.jsxs)(s.ol,{children:["\n",(0,o.jsx)(s.li,{children:(0,o.jsx)(s.a,{href:"#performance",children:"Performance (throughput)"})}),"\n",(0,o.jsx)(s.li,{children:(0,o.jsx)(s.a,{href:"#grouped-message-processing",children:"Grouped message processing"})}),"\n",(0,o.jsx)(s.li,{children:(0,o.jsx)(s.a,{href:"#compatibility",children:"Compatibility (mixing multi and single part message protocols)"})}),"\n"]}),"\n",(0,o.jsx)(s.h2,{id:"performance",children:"Performance"}),"\n",(0,o.jsxs)(s.p,{children:["For most users the only benefit of batching messages is improving throughput over your output protocol. For some protocols this can happen in the background and requires no configuration from you. However, if an output has a ",(0,o.jsx)(s.code,{children:"batching"})," configuration block this means it benefits from batching and requires you to specify how you'd like your batches to be formed by configuring a ",(0,o.jsx)(s.a,{href:"#batch-policy",children:"batching policy"}),":"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:"output:\n  kafka:\n    addresses: [ todo:9092 ]\n    topic: bento_stream\n\n    # Either send batches when they reach 10 messages or when 100ms has passed\n    # since the last batch.\n    batching:\n      count: 10\n      period: 100ms\n"})}),"\n",(0,o.jsxs)(s.p,{children:["However, a small number of inputs such as ",(0,o.jsx)(s.a,{href:"/docs/components/inputs/kafka",children:(0,o.jsx)(s.code,{children:"kafka"})})," must be consumed sequentially (in this case by partition) and therefore benefit from specifying your batch policy at the input level instead:"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:"input:\n  kafka:\n    addresses: [ todo:9092 ]\n    topics: [ bento_input_stream ]\n    batching:\n      count: 10\n      period: 100ms\n\noutput:\n  kafka:\n    addresses: [ todo:9092 ]\n    topic: bento_stream\n"})}),"\n",(0,o.jsxs)(s.p,{children:["Inputs that behave this way are documented as such and have a ",(0,o.jsx)(s.code,{children:"batching"})," configuration block."]}),"\n",(0,o.jsxs)(s.p,{children:["Sometimes you may prefer to create your batches before processing in order to benefit from ",(0,o.jsx)(s.a,{href:"#grouped-message-processing",children:"batch wide processing"}),", in which case if your input doesn't already support ",(0,o.jsx)(s.a,{href:"#batch-policy",children:"a batch policy"})," you can instead use a ",(0,o.jsx)(s.a,{href:"/docs/components/inputs/broker",children:(0,o.jsx)(s.code,{children:"broker"})}),", which also allows you to combine inputs with a single batch policy:"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:"input:\n  broker:\n    inputs:\n      - resource: foo\n      - resource: bar\n    batching:\n      count: 50\n      period: 500ms\n"})}),"\n",(0,o.jsxs)(s.p,{children:["This also works the same with ",(0,o.jsx)(s.a,{href:"/docs/components/outputs/broker",children:"output brokers"}),"."]}),"\n",(0,o.jsx)(s.h2,{id:"grouped-message-processing",children:"Grouped Message Processing"}),"\n",(0,o.jsxs)(s.p,{children:["And some processors such as ",(0,o.jsx)(s.a,{href:"/docs/components/processors/while",children:(0,o.jsx)(s.code,{children:"while"})})," are executed once across a whole batch, you can avoid this behaviour with the ",(0,o.jsxs)(s.a,{href:"/docs/components/processors/for_each",children:[(0,o.jsx)(s.code,{children:"for_each"})," processor"]}),":"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:"pipeline:\n  processors:\n    - for_each:\n      - while:\n          at_least_once: true\n          max_loops: 0\n          check: errored()\n          processors:\n            - catch: [] # Wipe any previous error\n            - resource: foo # Attempt this processor until success\n"})}),"\n",(0,o.jsxs)(s.p,{children:["There's a vast number of processors that specialise in operations across batches such as ",(0,o.jsx)(s.a,{href:"/docs/components/processors/group_by",children:"grouping"})," and ",(0,o.jsx)(s.a,{href:"/docs/components/processors/archive",children:"archiving"}),". For example, the following processors group a batch of messages according to a metadata field and compresses them into separate ",(0,o.jsx)(s.code,{children:".tar.gz"})," archives:"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:'pipeline:\n  processors:\n    - group_by_value:\n        value: ${! meta("kafka_partition") }\n    - archive:\n        format: tar\n    - compress:\n        algorithm: gzip\n\noutput:\n  aws_s3:\n    bucket: TODO\n    path: docs/${! meta("kafka_partition") }/${! count("files") }-${! timestamp_unix_nano() }.tar.gz\n'})}),"\n",(0,o.jsxs)(s.p,{children:["For more examples of batched (or windowed) processing check out ",(0,o.jsx)(s.a,{href:"/docs/configuration/windowed_processing",children:"this document"}),"."]}),"\n",(0,o.jsx)(s.h2,{id:"compatibility",children:"Compatibility"}),"\n",(0,o.jsx)(s.p,{children:"Bento is able to read and write over protocols that support multiple part messages, and all payloads travelling through Bento are represented as a multiple part message. Therefore, all components within Bento are able to work with multiple parts in a message as standard."}),"\n",(0,o.jsxs)(s.p,{children:["When messages reach an output that ",(0,o.jsx)(s.em,{children:"doesn't"})," support multiple parts the message is broken down into an individual message per part, and then one of two behaviours happen depending on the output. If the output supports batch sending messages then the collection of messages are sent as a single batch. Otherwise, Bento falls back to sending the messages sequentially in multiple, individual requests."]}),"\n",(0,o.jsx)(s.p,{children:"This behaviour means that not only can multiple part message protocols be easily matched with single part protocols, but also the concept of multiple part messages and message batches are interchangeable within Bento."}),"\n",(0,o.jsx)(s.h3,{id:"shrinking-batches",children:"Shrinking Batches"}),"\n",(0,o.jsxs)(s.p,{children:["A message batch (or multiple part message) can be broken down into smaller batches using the ",(0,o.jsx)(s.a,{href:"/docs/components/processors/split",children:(0,o.jsx)(s.code,{children:"split"})})," processor:"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:"input:\n  # Consume messages that arrive in three parts.\n  resource: foo\n  processors:\n    # Drop the third part\n    - select_parts:\n        parts: [ 0, 1 ]\n    # Then break our message parts into individual messages\n    - split:\n        size: 1\n"})}),"\n",(0,o.jsx)(s.p,{children:"This is also useful when your input source creates batches that are too large for your output protocol:"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:"input:\n  aws_s3:\n    bucket: todo\n\npipeline:\n  processors:\n    - decompress:\n        algorithm: gzip\n    - unarchive:\n        format: tar\n    # Limit batch sizes to 5MB\n    - split:\n        byte_size: 5_000_000\n"})}),"\n",(0,o.jsx)(s.h2,{id:"batch-policy",children:"Batch Policy"}),"\n",(0,o.jsxs)(s.p,{children:["When an input or output component has a config field ",(0,o.jsx)(s.code,{children:"batching"})," that means it supports a batch policy. This is a mechanism that allows you to configure exactly how your batching should work on messages before they are routed to the input or output it's associated with. Batches are considered complete and will be flushed downstream when either of the following conditions are met:"]}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsxs)(s.li,{children:["The ",(0,o.jsx)(s.code,{children:"byte_size"})," field is non-zero and the total size of the batch in bytes matches or exceeds it (disregarding metadata.)"]}),"\n",(0,o.jsxs)(s.li,{children:["The ",(0,o.jsx)(s.code,{children:"count"})," field is non-zero and the total number of messages in the batch matches or exceeds it."]}),"\n",(0,o.jsxs)(s.li,{children:["A message added to the batch causes the ",(0,o.jsx)(s.a,{href:"/docs/guides/bloblang/about",children:(0,o.jsx)(s.code,{children:"check"})})," to return to ",(0,o.jsx)(s.code,{children:"true"}),"."]}),"\n",(0,o.jsxs)(s.li,{children:["The ",(0,o.jsx)(s.code,{children:"period"})," field is non-empty and the time since the last batch exceeds its value."]}),"\n"]}),"\n",(0,o.jsx)(s.p,{children:"This allows you to combine conditions:"}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:"output:\n  kafka:\n    addresses: [ todo:9092 ]\n    topic: bento_stream\n\n    # Either send batches when they reach 10 messages or when 100ms has passed\n    # since the last batch.\n    batching:\n      count: 10\n      period: 100ms\n"})}),"\n",(0,o.jsx)(s.admonition,{type:"caution",children:(0,o.jsxs)(s.p,{children:["A batch policy has the capability to ",(0,o.jsx)(s.em,{children:"create"})," batches, but not to break them down."]})}),"\n",(0,o.jsxs)(s.p,{children:["If your configured pipeline is processing messages that are batched ",(0,o.jsx)(s.em,{children:"before"})," they reach the batch policy then they may circumvent the conditions you've specified here, resulting in sizes you aren't expecting."]}),"\n",(0,o.jsxs)(s.p,{children:["If you are affected by this limitation then consider breaking the batches down with a ",(0,o.jsxs)(s.a,{href:"/docs/components/processors/split",children:[(0,o.jsx)(s.code,{children:"split"})," processor"]})," before they reach the batch policy."]}),"\n",(0,o.jsx)(s.h3,{id:"post-batch-processing",children:"Post-Batch Processing"}),"\n",(0,o.jsxs)(s.p,{children:["A batch policy also has a field ",(0,o.jsx)(s.code,{children:"processors"})," which allows you to define an optional list of ",(0,o.jsx)(s.a,{href:"/docs/components/processors/about",children:"processors"})," to apply to each batch before it is flushed. This is a good place to aggregate or archive the batch into a compatible format for an output:"]}),"\n",(0,o.jsx)(s.pre,{children:(0,o.jsx)(s.code,{className:"language-yaml",children:"output:\n  http_client:\n    url: http://localhost:4195/post\n    batching:\n      count: 10\n      processors:\n        - archive:\n            format: lines\n"})}),"\n",(0,o.jsxs)(s.p,{children:["The above config will batch up messages and then merge them into a line delimited format before sending it over HTTP. This is an easier format to parse than the default which would have been ",(0,o.jsx)(s.a,{href:"https://www.w3.org/Protocols/rfc1341/7_2_Multipart.html",children:"rfc1342"}),"."]}),"\n",(0,o.jsx)(s.p,{children:"During shutdown any remaining messages waiting for a batch to complete will be flushed down the pipeline."})]})}function d(e={}){const{wrapper:s}={...(0,t.a)(),...e.components};return s?(0,o.jsx)(s,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},11151:function(e,s,n){n.d(s,{Z:function(){return a},a:function(){return r}});var o=n(67294);const t={},i=o.createContext(t);function r(e){const s=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(i.Provider,{value:s},e.children)}}}]);